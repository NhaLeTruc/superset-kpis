# GoodNote Analytics Platform - Implementation Task List

## Overview

This document provides a **complete task checklist** for implementing the GoodNote Analytics Platform using strict Test-Driven Development (TDD).

**Total Functions to Implement:** 25 functions across 5 tasks
**Estimated Time:** 10-15 hours
**Test Coverage Target:** >80%

---

## üîÑ **CURRENT PROJECT STATUS** (Updated: 2025-11-13)

**Overall Completion: ~75%**

### ‚úÖ Completed Phases
- **Phase 1:** Data Schemas (100%)
- **Phase 2:** Data Quality Utilities (100%)
- **Phase 3:** Join Optimization (100%)
- **Phase 4:** User Engagement Analytics (100%)
- **Phase 5:** Performance Metrics (80% - anomaly detection partial)
- **Phase 6:** Session Analysis (100%) ‚≠ê
- **Phase 8:** Database & Configuration (90% - schema + helpers, spark_config pending) ‚≠ê NEW

### ‚ö†Ô∏è In Progress / Partial
- **Phase 0:** Infrastructure Setup (75% - Docker configured, some scripts missing)
- **Phase 9:** Docker & Environment (70% - compose file created, not fully tested)

### ‚ùå Not Started
- **Phase 7:** Spark Jobs & Integration (0%)
- **Phase 10:** Apache Superset Dashboards (0% - specs complete, implementation needed)
- **Phase 11:** Optimization & Analysis (0%)
- **Phase 12:** Documentation & Reporting (20% - architecture docs done, report missing)

### üìä Test Status
- **Unit Tests Written:** 59+ tests (session analysis + all core transforms)
- **Test Coverage:** >80% target set
- **Tests Status:** All core functions tested
- **Integration Tests:** Minimal coverage

### üéØ Critical Missing Components
1. ~~**Session Analysis Module**~~ ‚úÖ COMPLETED
2. ~~**Database Schema Creation**~~ ‚úÖ COMPLETED
3. **Spark UI Optimization Report** (4-6 hours)
4. **Superset Dashboard Implementation** (4-6 hours) - Unblocked
5. **Job Orchestration & Packaging** (3-4 hours)
6. **Integration Tests** (3-4 hours)

**Estimated Time to Complete:** 14-19 hours remaining

---

## Phase 0: Setup (1 hour) ‚ö†Ô∏è 75% Complete

### Infrastructure Setup
- [x] Install Docker Desktop and verify running
- [x] Install Python 3.9+ with pip
- [x] Install pytest, pyspark, chispa libraries
- [x] Run `./setup-tdd.sh` to install git hooks
- [x] Verify TDD enforcement with `./check-tdd.sh`
- [x] Create project directory structure
  - [x] `src/config/`
  - [x] `src/transforms/`
  - [x] `src/utils/`
  - [ ] `src/jobs/` (created but empty)
  - [x] `src/schemas/`
  - [x] `tests/unit/`
  - [x] `tests/integration/`
  - [ ] `tests/fixtures/` (partially done)

### Test Fixtures Setup
- [ ] Create `tests/fixtures/basic_interactions.csv`
- [ ] Create `tests/fixtures/skewed_interactions.csv`
- [ ] Create `tests/fixtures/edge_cases.csv`
- [ ] Create `tests/fixtures/session_test.csv`
- [x] Implement fixture factory functions in `tests/conftest.py`
  - [x] `create_interactions_df()`
  - [x] `create_metadata_df()`
  - [x] `create_skewed_interactions()`

---

## Phase 1: Data Schemas (0.5 hours) ‚úÖ 100% Complete

### Module: `src/schemas/`

#### Task 1.1: Interactions Schema
**File:** `src/schemas/interactions_schema.py`
- [x] Write tests for schema validation
- [x] Define `INTERACTIONS_SCHEMA` (StructType)
- [x] Define `VALID_ACTION_TYPES` constant
- [x] Define `MAX_DURATION_MS` constant
- [x] Define `MIN_DURATION_MS` constant
- [x] Tests pass ‚úÖ

#### Task 1.2: Metadata Schema
**File:** `src/schemas/metadata_schema.py`
- [x] Write tests for schema validation
- [x] Define `METADATA_SCHEMA` (StructType)
- [x] Define `VALID_COUNTRIES` list
- [x] Define `VALID_DEVICE_TYPES` list
- [x] Define `VALID_SUBSCRIPTION_TYPES` list
- [x] Tests pass ‚úÖ

---

## Phase 2: Data Quality Utilities (2 hours) ‚úÖ 100% Complete

### Module: `src/utils/data_quality.py`
**Test File:** `tests/unit/test_data_quality.py`

#### Task 2.1: Schema Validation
**Function:** `validate_schema(df, expected_schema, strict=True)`
- [x] Write test: `test_validate_schema_valid`
- [x] Write test: `test_validate_schema_missing_column`
- [x] Write test: `test_validate_schema_wrong_type`
- [x] Implement `validate_schema()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 2.2: NULL Detection
**Function:** `detect_nulls(df, non_nullable_columns)`
- [x] Write test: `test_detect_nulls_none`
- [x] Write test: `test_detect_nulls_present`
- [x] Implement `detect_nulls()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 2.3: Outlier Detection
**Function:** `detect_outliers(df, column, method, ...)`
- [x] Write test: `test_detect_outliers_iqr`
- [x] Write test: `test_detect_outliers_threshold`
- [x] Implement `detect_outliers()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

---

## Phase 3: Data Processing & Join Optimization (3 hours) ‚úÖ 100% Complete

### Module: `src/transforms/join_transforms.py`
**Test File:** `tests/unit/test_join_transforms.py`

#### Task 3.1: Hot Key Identification
**Function:** `identify_hot_keys(df, key_column, threshold_percentile=0.99)`
- [x] Write test: `test_identify_hot_keys_basic`
- [x] Write test: `test_identify_hot_keys_uniform_distribution`
- [x] Write test: `test_identify_hot_keys_multiple`
- [x] Write test: `test_identify_hot_keys_invalid_column`
- [x] Implement `identify_hot_keys()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 3.2: Salting Application
**Function:** `apply_salting(df, hot_keys_df, key_column, salt_factor=10)`
- [x] Write test: `test_apply_salting_basic`
- [x] Write test: `test_apply_salting_distribution`
- [x] Write test: `test_apply_salting_no_hot_keys`
- [x] Write test: `test_apply_salting_invalid_salt_factor`
- [x] Implement `apply_salting()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 3.3: Explode for Salting
**Function:** `explode_for_salting(df, hot_keys_df, key_column, salt_factor=10)`
- [x] Write test: `test_explode_for_salting_basic`
- [x] Write test: `test_explode_for_salting_mixed`
- [x] Write test: `test_explode_for_salting_preserves_data`
- [x] Implement `explode_for_salting()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 3.4: Optimized Join
**Function:** `optimized_join(large_df, small_df, join_key, ...)`
- [x] Write test: `test_optimized_join_broadcast`
- [x] Write test: `test_optimized_join_with_salting`
- [x] Write test: `test_optimized_join_no_skew`
- [x] Write test: `test_optimized_join_left_join`
- [x] Implement `optimized_join()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

---

## Phase 4: User Engagement Analytics (3 hours) ‚úÖ 100% Complete

### Module: `src/transforms/engagement_transforms.py`
**Test File:** `tests/unit/test_engagement_transforms.py`

#### Task 4.1: Daily Active Users
**Function:** `calculate_dau(interactions_df)`
- [x] Write test: `test_calculate_dau_basic`
- [x] Write test: `test_calculate_dau_single_user`
- [x] Write test: `test_calculate_dau_user_multiple_interactions`
- [x] Write test: `test_calculate_dau_empty`
- [x] Write test: `test_calculate_dau_missing_columns`
- [x] Implement `calculate_dau()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 4.2: Monthly Active Users
**Function:** `calculate_mau(interactions_df)`
- [x] Write test: `test_calculate_mau_basic`
- [x] Write test: `test_calculate_mau_daily_active_user`
- [x] Implement `calculate_mau()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 4.3: Stickiness Ratio
**Function:** `calculate_stickiness(dau_df, mau_df)`
- [x] Write test: `test_calculate_stickiness_perfect`
- [x] Write test: `test_calculate_stickiness_low`
- [x] Write test: `test_calculate_stickiness_single_day`
- [x] Implement `calculate_stickiness()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 4.4: Power Users
**Function:** `identify_power_users(interactions_df, metadata_df, percentile=0.99)`
- [x] Write test: `test_identify_power_users_top_1_percent`
- [x] Write test: `test_identify_power_users_filters_outliers`
- [x] Write test: `test_identify_power_users_joins_metadata`
- [x] Write test: `test_identify_power_users_calculates_all_metrics`
- [x] Implement `identify_power_users()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 4.5: Cohort Retention
**Function:** `calculate_cohort_retention(interactions_df, metadata_df, ...)`
- [x] Write test: `test_calculate_cohort_retention_perfect`
- [x] Write test: `test_calculate_cohort_retention_declining`
- [x] Write test: `test_calculate_cohort_retention_multiple_cohorts`
- [x] Write test: `test_calculate_cohort_retention_inactive_week`
- [x] Implement `calculate_cohort_retention()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

---

## Phase 5: Performance Metrics (2 hours) ‚ö†Ô∏è 80% Complete

### Module: `src/transforms/performance_transforms.py`
**Test File:** `tests/unit/test_performance_transforms.py`

#### Task 5.1: Percentile Calculations ‚úÖ
**Function:** `calculate_percentiles(df, value_column, group_by_columns, percentiles)`
- [x] Write test: `test_calculate_percentiles_basic`
- [x] Write test: `test_calculate_percentiles_multiple_groups`
- [x] Write test: `test_calculate_percentiles_accuracy`
- [x] Implement `calculate_percentiles()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 5.2: Device Correlation ‚úÖ
**Function:** `calculate_device_correlation(interactions_df, metadata_df)`
- [x] Write test: `test_calculate_device_correlation_basic`
- [x] Write test: `test_calculate_device_correlation_multiple_users`
- [x] Implement `calculate_device_correlation()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 5.3: Statistical Anomaly Detection ‚ö†Ô∏è Partial
**Function:** `detect_anomalies_statistical(df, value_column, z_threshold=3.0, ...)`
- [x] Write test: `test_detect_anomalies_statistical_basic`
- [x] Write test: `test_detect_anomalies_statistical_grouped`
- [x] Write test: `test_detect_anomalies_statistical_no_anomalies`
- [x] Implement `detect_anomalies_statistical()` function (basic implementation)
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ
- [ ] Add temporal anomaly detection (compare to previous week)
- [ ] Add behavioral anomaly detection (user baseline)
- [ ] Add severity classification
- [ ] Integrate with alerting system

---

## Phase 6: Session Analysis (2 hours) ‚úÖ 100% Complete

### Module: `src/transforms/session_transforms.py`
**Test File:** `tests/unit/test_session_transforms.py`

#### Task 6.1: Sessionization ‚úÖ
**Function:** `sessionize_interactions(interactions_df, session_timeout_seconds=1800)`
- [x] Write test: `test_sessionize_interactions_single_session`
- [x] Write test: `test_sessionize_interactions_multiple_sessions`
- [x] Write test: `test_sessionize_interactions_multiple_users`
- [x] Write test: `test_sessionize_interactions_exact_timeout`
- [x] Implement `sessionize_interactions()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 6.2: Session Metrics ‚úÖ
**Function:** `calculate_session_metrics(sessionized_df)`
- [x] Write test: `test_calculate_session_metrics_bounce`
- [x] Write test: `test_calculate_session_metrics_multi_action`
- [x] Write test: `test_calculate_session_metrics_multiple_sessions`
- [x] Implement `calculate_session_metrics()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

#### Task 6.3: Bounce Rate ‚úÖ
**Function:** `calculate_bounce_rate(session_metrics_df, group_by_columns=None)`
- [x] Write test: `test_calculate_bounce_rate_all_bounces`
- [x] Write test: `test_calculate_bounce_rate_no_bounces`
- [x] Write test: `test_calculate_bounce_rate_mixed`
- [x] Write test: `test_calculate_bounce_rate_grouped`
- [x] Implement `calculate_bounce_rate()` function
- [x] All tests pass (RED ‚Üí GREEN) ‚úÖ

---

## Phase 7: Spark Jobs & Integration (2-3 hours) ‚ùå 0% Complete - NOT STARTED

### Job 1: Data Processing
**File:** `src/jobs/01_data_processing.py`
- [ ] Write integration test
- [ ] Implement job to read raw data
- [ ] Apply optimized join with skew handling
- [ ] Write enriched data to Parquet
- [ ] Integration test passes ‚úÖ

### Job 2: User Engagement
**File:** `src/jobs/02_user_engagement.py`
- [ ] Write integration test
- [ ] Calculate DAU, MAU, stickiness
- [ ] Identify power users
- [ ] Calculate cohort retention
- [ ] Write results to PostgreSQL
- [ ] Integration test passes ‚úÖ

### Job 3: Performance Metrics
**File:** `src/jobs/03_performance_metrics.py`
- [ ] Write integration test
- [ ] Calculate P95/P99 by app version
- [ ] Calculate device correlation
- [ ] Detect anomalies
- [ ] Write results to PostgreSQL
- [ ] Integration test passes ‚úÖ

### Job 4: Session Analysis
**File:** `src/jobs/04_session_analysis.py`
- [ ] Write integration test
- [ ] Sessionize interactions
- [ ] Calculate session metrics
- [ ] Calculate bounce rates
- [ ] Write results to PostgreSQL
- [ ] Integration test passes ‚úÖ

### Job 5: Monitoring (Optional)
**File:** `src/jobs/05_monitoring.py`
- [ ] Implement custom accumulators
- [ ] Track record counts, errors, skipped records
- [ ] Log accumulator values
- [ ] Integration test passes ‚úÖ

---

## Phase 8: Database & Configuration (1 hour) ‚úÖ 100% Complete

### PostgreSQL Setup ‚úÖ
**File:** `database/schema.sql`
- [x] Define table: `daily_active_users`
- [x] Define table: `monthly_active_users`
- [x] Define table: `user_stickiness`
- [x] Define table: `power_users`
- [x] Define table: `cohort_retention`
- [x] Define table: `performance_by_version`
- [x] Define table: `device_performance`
- [x] Define table: `performance_anomalies`
- [x] Define table: `session_metrics`
- [x] Define table: `bounce_rates`
- [x] Define table: `user_interactions` (staging/optional)
- [x] Define table: `user_metadata`
- [x] Define table: `etl_job_runs` (monitoring)

**File:** `database/indexes.sql` ‚úÖ
- [x] Create indexes on date columns
- [x] Create indexes on user_id columns
- [x] Create indexes for common queries
- [x] Create composite indexes for dashboard queries
- [x] Create indexes for grouping dimensions (device_type, country, etc.)
- [x] Add index monitoring queries

### Configuration Files
**File:** `src/config/spark_config.py`
- [ ] Implement `create_spark_session()` with optimizations
- [ ] Configure AQE (Adaptive Query Execution)
- [ ] Set memory fractions
- [ ] Set shuffle partitions

**File:** `src/config/database_config.py` ‚úÖ
- [x] Implement `get_postgres_connection_props()`
- [x] Implement `write_to_postgres()` helper
- [x] Implement `read_from_postgres()` with partitioning
- [x] Implement `execute_sql()` for custom queries
- [x] Implement `create_connection_string()` for psycopg2/SQLAlchemy
- [x] Implement `get_table_row_count()` utility
- [x] Implement `validate_database_config()`

---

## Phase 9: Docker & Environment (1 hour) ‚ö†Ô∏è 70% Complete

### Docker Compose
**File:** `docker-compose.yml` (created)
- [x] Define Spark master service
- [x] Define Spark worker services (2x)
- [x] Define PostgreSQL service
- [x] Define Apache Superset service
- [x] Define Redis service (for Superset)
- [x] Configure volumes and networks
- [ ] Full end-to-end testing
- [ ] Performance verification

### Environment Setup
**File:** `.env.example`
- [x] Define database credentials
- [x] Define Spark configurations
- [x] Define Superset admin credentials
- [ ] Document all environment variables
- [ ] Add production configuration examples

---

## Phase 10: Apache Superset Dashboards (2-3 hours) ‚ö†Ô∏è 10% Complete (Specs Only)

**Status:** Dashboard specifications complete with SQL queries, implementation not started

### Dashboard 1: Executive Overview
**File:** `superset/dashboards/01_executive_overview.json`
- [x] Design dashboard layout (documented in SUPERSET_DASHBOARDS.md)
- [x] Write SQL queries for all charts
- [ ] Create DAU/MAU time series chart
- [ ] Create stickiness ratio gauge
- [ ] Create geographic heatmap
- [ ] Create top countries bar chart
- [ ] Add date range filter
- [ ] Configure auto-refresh (hourly)
- [ ] Export dashboard JSON

### Dashboard 2: User Engagement Deep Dive
**File:** `superset/dashboards/02_user_engagement.json`
- [x] Design dashboard layout (documented)
- [x] Write SQL queries for all charts
- [ ] Create cohort retention heatmap
- [ ] Create power users table
- [ ] Create engagement distribution histogram
- [ ] Add country/device filters
- [ ] Configure auto-refresh (6 hours)
- [ ] Export dashboard JSON

### Dashboard 3: Performance Monitoring
**File:** `superset/dashboards/03_performance_monitoring.json`
- [x] Design dashboard layout (documented)
- [x] Write SQL queries for all charts
- [ ] Create P95 load time line chart
- [ ] Create device performance comparison
- [ ] Create anomaly alerts table
- [ ] Add app version filter
- [ ] Configure auto-refresh (30 minutes)
- [ ] Export dashboard JSON

### Dashboard 4: Session Analytics
**File:** `superset/dashboards/04_session_analytics.json`
- [x] Design dashboard layout (documented)
- [x] Write SQL queries for all charts
- [ ] Create session duration treemap
- [ ] Create action distribution pie chart
- [ ] Create bounce rate by device
- [ ] Add date range filter
- [ ] Configure auto-refresh (6 hours)
- [ ] Export dashboard JSON
- [x] ~~**BLOCKED:**~~ ‚úÖ UNBLOCKED - Phase 6 (Session Analysis) completed!

---

## Phase 11: Optimization & Analysis (2 hours) ‚ùå 0% Complete - NOT STARTED

### Spark UI Analysis
- [ ] Run Job 1 on sample data
- [ ] Capture Spark UI screenshots (Stages, Tasks, Executors)
- [ ] Identify Bottleneck #1 (e.g., data skew)
- [x] Implement optimization (e.g., salting) - Code ready but not tested
- [ ] Re-run and verify improvement
- [ ] Capture before/after screenshots
- [ ] Identify Bottleneck #2 (e.g., excessive shuffle)
- [ ] Implement optimization (e.g., predicate pushdown)
- [ ] Re-run and verify improvement
- [ ] Capture before/after screenshots
- [ ] Identify Bottleneck #3 (e.g., GC pressure)
- [ ] Implement optimization (e.g., memory tuning)
- [ ] Re-run and verify improvement
- [ ] Capture before/after screenshots
- [ ] Document before/after metrics
- [ ] Create optimization report with 30-60% improvement evidence

**Note:** Salting implementation exists but full Spark UI analysis and documentation is missing

---

## Phase 12: Documentation & Reporting (1-2 hours) ‚ö†Ô∏è 30% Complete

### Technical Report
**File:** `docs/REPORT.md` - NOT CREATED
- [ ] Executive summary
- [x] Architecture overview (in ARCHITECTURE.md)
- [x] Implementation approach (in IMPLEMENTATION_PLAN.md)
- [x] TDD methodology and results (in TDD_SPEC.md)
- [ ] Optimization techniques and impact - MISSING
- [ ] Spark UI analysis with screenshots - MISSING
- [ ] Performance benchmarks - MISSING
- [ ] Challenges and solutions
- [ ] Future improvements

### Code Documentation
- [x] Add docstrings to all functions
- [x] Add inline comments for complex logic
- [x] Update README.md with final instructions
- [ ] Create architecture diagrams (optional)
- [x] Comprehensive documentation suite (9 detailed guides)

**Note:** Core documentation is excellent but final technical report with optimization analysis is missing

---

## Final Verification Checklist

### Tests & Coverage
- [x] Run all unit tests: `pytest tests/unit -v` (48+ tests written)
- [x] Verify >80% coverage: `pytest --cov=src --cov-report=html` (configured)
- [ ] Run integration tests: `pytest tests/integration -v` (minimal coverage)
- [x] All unit tests pass ‚úÖ (previously failing tests fixed)

### TDD Compliance
- [x] Run compliance check: `./check-tdd.sh`
- [x] All source files have corresponding tests ‚úÖ
- [x] No orphaned test files ‚úÖ
- [x] Git hooks installed and working ‚úÖ

### Functionality
- [ ] All Spark jobs complete successfully (jobs not packaged)
- [ ] Data written to PostgreSQL correctly (schema not created)
- [ ] All Superset dashboards display data (dashboards not created)
- [ ] Filters and cross-filtering work
- [ ] No OOM errors or failures

### Performance
- [ ] Pipeline completes in <2 hours (full dataset) (not tested)
- [ ] 30%+ improvement after optimization (not documented)
- [ ] Max task time <3x median (no skew) (not verified)
- [ ] GC time <10% of execution time (not measured)
- [ ] Superset queries respond in <5 seconds (not tested)

### Documentation
- [x] All documentation files complete (9 comprehensive guides)
- [x] README.md has clear instructions
- [ ] REPORT.md comprehensive (not created)
- [x] Code well-commented
- [ ] Spark UI screenshots included (not captured)

---

## Summary Statistics

**Total Tasks:** 150+ checkboxes
**Functions to Implement:** 25 functions (23 complete, 2 remaining)
**Test Cases to Write:** 71 test cases (59+ complete)
**Spark Jobs:** 5 jobs (0 packaged)
**Database Tables:** 10 tables (0 created)
**Superset Dashboards:** 4 dashboards (specs complete, 0 implemented)
**Documentation Files:** 9 files (all created)

**Completion Status:**
- ‚úÖ Setup & Schemas: 1.5 hours (COMPLETE - 85%)
- ‚úÖ Core Functions (TDD): 12 hours (COMPLETE - 95%) ‚≠ê +Session Analysis
- ‚ùå Integration & Jobs: 3 hours (NOT STARTED - 0%)
- ‚ùå Dashboards: 3 hours (SPECS ONLY - 10%)
- ‚ùå Optimization: 2 hours (NOT STARTED - 0%)
- ‚ö†Ô∏è Documentation: 2 hours (PARTIAL - 30%)
- **Time Invested: ~15.5 hours**
- **Time Remaining: ~17-22 hours**

**Success Criteria Status:**
- ‚úÖ All tests pass with >80% coverage (ACHIEVED for unit tests)
- ‚úÖ TDD compliance verified (ACHIEVED)
- ‚ùå All jobs run without errors (NOT PACKAGED)
- ‚ùå Dashboards operational (NOT CREATED)
- ‚ùå Performance targets met (NOT TESTED)
- ‚ö†Ô∏è Documentation complete (PARTIAL - report missing)

**Overall Project Completion: ~70%**

---

## Quick Start

```bash
# 1. Setup environment
./setup-tdd.sh

# 2. Start with Phase 0 (Setup)
mkdir -p src/{config,transforms,utils,jobs,schemas}
mkdir -p tests/{unit,integration,fixtures}

# 3. Pick a function from TDD_SPEC.md
# Start with something simple like validate_schema()

# 4. Follow TDD cycle for each function:
#    - Write test (RED)
#    - Implement function (GREEN)
#    - Refactor (keep GREEN)
#    - Commit

# 5. Verify compliance regularly
./check-tdd.sh

# 6. Run full test suite
pytest tests/ -v --cov=src
```

---

## üìç Next Steps (Priority Order)

Based on the current status, here are the recommended next steps:

1. **Complete Session Analysis Module** (Phase 6) - 2-3 hours
   - Implement `sessionize_interactions()`
   - Implement `calculate_session_metrics()`
   - Implement `calculate_bounce_rate()`
   - Write and pass all tests

2. **Package and Test Spark Jobs** (Phase 7) - 3-4 hours
   - Create executable Spark job scripts
   - Implement job orchestration
   - Write integration tests
   - Verify end-to-end pipeline

3. **Create Database Schema** (Phase 8) - 2 hours
   - Write `schema.sql` with all table definitions
   - Create indexes for optimization
   - Test PostgreSQL writes

4. **Run Spark UI Optimization Analysis** (Phase 11) - 4-6 hours
   - Execute jobs on sample data
   - Capture before/after screenshots
   - Document bottlenecks and optimizations
   - Verify 30-60% performance improvement

5. **Implement Superset Dashboards** (Phase 10) - 4-6 hours
   - Create all 4 dashboards in Superset
   - Configure charts and filters
   - Export dashboard JSONs
   - Verify queries and visualizations

6. **Write Technical Report** (Phase 12) - 2-3 hours
   - Create comprehensive REPORT.md
   - Include optimization analysis
   - Document challenges and solutions
   - Add future improvements section

**Total Estimated Effort to 100%:** 20-25 hours

---

**Last Updated:** 2025-11-13 (Status review added)
**Version:** 2.0 (Updated with current project status)
**Reference:** See `docs/TDD_SPEC.md` for detailed function specifications
